<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>dPCA</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">


# Demixing and summarising neural activity 

</script></section><section ><section data-markdown><script type="text/template">



## Introduction

<font size=10>

We have data (spike trains). Loads of it. 

</font>

**What do we do???**

</script></section><section data-markdown><script type="text/template">

### What is "demixing"? 

<font size=6>

_mixed selectivity_: individual neurons are modulated by multiple task parameters (Raposo Kaufman Churchland (2014))

we want to _demix_ at the level of: 

 - individual neurons: neuron codes for both $x$ and $y$,  
 - population:
  - part of population codes for $x$, another part codes for $y$
  - OR all individual neurons have mixed selectivity (a.k.a _mess_)
  
</font>   

</script></section><section data-markdown><script type="text/template">

### What is "summarising"?

 - we want to describe the dynamics of neural activity in lower dimensions 
 - ie. explains variability of the data: both variability due to task parameters and intrinsic stochasticity (the neuron is irregular)
 - you want to reconstruct your data using that lower-dim representation (_interpretability_)

</script></section></section><section ><section data-markdown><script type="text/template">



## Approaches

| Method | Demixes? | Summarises? | Cool? |
|---------|---------|----------|--|
| statistical test | Yes | No | No.. |
| LDA | Yes | No | Meh |
| PCA, GPFA, ... | No | Yes | I guess |
| dPCA | Yes | Yes | <p>&#129321;</p> |

We'll focus on [Kobak et al 2016: Demixed principal component analysis of neural population vector](https://elifesciences.org/articles/10989)

</script></section><section data-markdown><script type="text/template">

### Traditional statistical tests

<font size=5>

- The 'standard' approach of neuroscience: perform a t-test (or any two-sample comparison test) of the firing rate of each neuron before and after the stimulus/response. This can be thought of as demixing. 
  - you find (threshold) $x$ % of neurons that respond to $a$ and $y$ % of neurons that respond to $b$ so that at each time point we 'know' the fraction of neurons responding to one variable or another
  - but that's single variable coding, we lose mixed selectivity, we lose saying what contribution in the neuron response comes from each variable
  - and we also don't account for population coding (different neurons coding same variable but in different ways)
  
a better approach will be to consider the activity of all neurons _together_
  
</font>

</script></section><section data-markdown><script type="text/template">

### PCA: Quick introduction

We want to summarise high dimensional neural activity in lower dimensions to make it easier to visualise / interpret the data. 

http://setosa.io/ev/principal-component-analysis/

</script></section><section data-markdown><script type="text/template">

### PCA: Quick introduction 

Matrices transform vectors. 

<iframe frameborder="0" width="100%" height="500pt" src="https://ncase.me/matrix/"></iframe>

</script></section><section data-markdown><script type="text/template">

### PCA: projections 

<div id='left'> 

A **non-square** matrix performs a projection when it transform a vector to a space with different dimension.

 - \# columns: dimension of your _input_ 
 - \# rows: dimension of your _output_
 
 </div>
 
<div id='right'> 

_Example_: transform vector by a  $1 \times 2$ matrix: $$A\vec{v} = \begin{bmatrix} 1 & 2 \end{bmatrix}\begin{bmatrix} v_1 \\\\ v_2\end{bmatrix}$$

![Projection matrix example](./figures/linear_projection_example.png) <!-- .element height="70%" width="70%"; -->


</div>

</script></section><section data-markdown><script type="text/template">

### PCA: objective 

PCA tries to find the projection matrix that minmise reconstruction error: 

$$
\mathcal{L}_\text{PCA} = \| \| X - D^\intercal D X \|\|^2
$$

where: 

 - $D$ is a fat short matrix (column > row)
 - $DX$ is the projection to your PC space 
 - $D^\intercal (DX)$ projects back (reconstruct) to the original space 
 
</script></section><section data-markdown><script type="text/template">

### PCA: demo 

<iframe frameborder="0" width="100%" height="500pt" src="http://setosa.io/ev/principal-component-analysis/"></iframe>

</script></section><section data-markdown><script type="text/template">

### dPCA 

<font size=5>

We start with our data $X$. It has dimensions $N \times KSQT$

<div id='left'> 

- $K$ trials 
- $S$ stimulus 
</div>
 
<div id='right'> 

- $Q$ decisions 
- $T$ time bins 
</div>
 
We decompose the activity of each neuron by the contribution of each experiment variable and their interactions: 

</font>
 
<font size=5>

`$$ 
x^i_{tdsk} = \overline{x} + \overline{x}_t + \overline{x}_s + \overline{x}_d + \overline{x}_{ts} + \overline{x}_{td} + \overline{x}_{sd} + \overline{x}_{tsd} + \varepsilon_{tdsk} 
$$`

`$$  
\stackrel{\tiny grouping}{=} \overline{x} + \overline{x}_t + \overline{x}_{ts} + \overline{x}_{td} + \overline{x}_{tsd} + \varepsilon_{tdsk} 
$$`
 
</font>
 
</script></section><section data-markdown><script type="text/template">

### dPCA: input data format

<div id='left'>

![Condition-decision matrix](./notebook/condition_decision_matrix.png) <!-- .element height="60%" width="90%"; -->

</div>

<div id='right'>

![Data matrix input for dPCA](./notebook/data_matrix.png) <!-- .element height="60%" width="90%"; -->

</div>

<font size=5>
where you have $N$ neurons, $K$ trials, $S$ stimulus, $D$ decisions and $T$ time points
</font>

</script></section><section data-markdown><script type="text/template">
 
### dPCA: taking the mean 

![Taking the mean across a dimension dPCA matrix](./notebook/matrices_tiled_in_3D.png) <!-- .element height="70%" width="70%"; -->

</script></section><section data-markdown><script type="text/template"> 
 
### dPCA: objective 

<font size=6>
We only want to reconstruct the contribution of each experimental variable individually.

Remember: the $X_\phi$ matrices are those averages, plus some residual contribution/noise
</font>

$$
X = \sum_{\phi = ( t, ts, td, tsd )} X_\phi + X_\text{res}
$$

</script></section><section data-markdown><script type="text/template">

### dPCA: objective 

<font size=6>

This can be done by having a separate decoder transformation matrix (PCA: same matrix to compress and map back to the original space):

</font>

$$
\mathcal{L}_\text{dPCA}^\phi = \vert\vert \mathbf{X}_\phi - \mathbf{F}_\phi\mathbf{D}_\phi\mathbf{X} \vert\vert^2
$$

<font size=5>

where: 

 - $\mathbf{D}_\phi$ is used to compress the data into a space with fewer dimensions 
 - $\mathbf{F}_\phi$ is used to map the data to the mean activity of interest (PCA: original data) 

</font>

</script></section><section data-markdown><script type="text/template">

Demixed PCA tries to balance two goals: demixing and summarising .

![Demixed PCA example](./figures/dPCA/fig-2-bdf.png) 

</script></section></section><section ><section data-markdown><script type="text/template">



## Results of dPCA

</script></section><section data-markdown><script type="text/template">

### Example of applying demixed PCA : task

Romo 1999: Monkeys compare frequency of two vibrations 

![Romo 1999 task](./figures/dPCA/romo-1999-fig-ab.png) <!-- .element height="70%" width="40%"; style="margin:auto;display:block"-->

</script></section><section data-markdown><script type="text/template">

### Example of applying demixed PCA : input data

 - Input data: we separate out our neural data based on stimulus condition: there are '6' possible stimulus conditions 
 - The input data is also separated out according to 2 decisions 
 - You then run dPCA through this data 
 
 </script></section><section data-markdown><script type="text/template">
 
### Example of applying demixed PCA : output of dPCA 

<font size=5>

Same as PCA, dPCA gives you the top $n$ principal components. 
But dPCA also gives you the experimental variable which the component explains most of the variability of neural activity: 

</font>

<div id='left'>
 
![Component and variance](./figures/dPCA/fig-3-cd.png) <!-- .element height="70%" width="70%";  -->

</div>

<div id='right'>

<font size=4>

1. Stimulus component: axis that best demixes the differences in neural activity due to differences in stimulus 
2. Decision component: differences in decision (and time) best demixes
3. Interaction component: variability due to interaction between stimulus and decision 
4. Condition-independent: does not depend on  particular stimulus / decision, but due to either factors that vary with time (eg. the fact that you are presenting the vibration from time $t_1$ to time $t_2$)
 
</font>
 
 </div> 


</script></section><section data-markdown><script type="text/template">

### Example of applying demixed PCA: looking at how each PC vary with time 

![Changes of projected neural activity over time](./figures/dPCA/fig-3-b.png) <!-- .element height="50%" width="50%"; style="margin:auto;display:block" -->

</script></section></section><section  data-markdown><script type="text/template"> 

### Caveats of dPCA

<font size=6>

It does not work if: 

 - you don't have all combinations of task parameters 
 - continuous task parameters 
 - you need a lot more neurons than your task parameters 
 
 
 There are workarounds for: 
 
  - your trials are unbalanced (re-balancing procedure)
  - different trial lengths (time warping)
</font> 

</script></section><section ><section data-markdown><script type="text/template">

## Other demixing / summarising methods

</script></section><section data-markdown><script type="text/template">


### Non-linear extension of dPCA using kernels (kdPCA)
[Latimer 2019: Nonlinear demixed component analysis for neural population data as a low-rank kernel regression problem](https://nbdt.scholasticahq.com/article/11523-nonlinear-demixed-component-analysis-for-neural-population-data-as-a-low-rank-kernel-regression-problem)

</script></section><section data-markdown><script type="text/template">

#### Why do we need nonlinear methods? 


![Latimer kernel PCA demo](./figures/dPCA/latimer2018-fig-2ab.png) <!-- .element height="70%" width="60%"; style="margin:auto;display:block" -->



</script></section><section data-markdown><script type="text/template">

<div id='left'>

Pros

 - it's non-linear: more flexibility 
 - (uses kernels)

</div>

<div id ='right'>

Cons 

 - extracted components are maybe less interpretable  
 - more parameter tuning (and so need more robust cross-validation)

</div>



</script></section><section data-markdown><script type="text/template">

### Tensor Component Analysis (TCA)
[Williams et al. 2018: Unsupervised Discovery of Demixed, Low-Dimensional Neural Dynamcis across Multiple Timescales through Tensor Component Analysis](https://www.sciencedirect.com/science/article/pii/S0896627318303878)

</script></section><section data-markdown><script type="text/template">


### What is tensor component analysis?

<div id='left'>

![Latimer kernel PCA demo](./figures/other-dim-reduce-methods/TCA-paper-figure-1.png) <!-- .element style="margin:auto;display:block" -->

</div>


<div id='right'>

<font size=5>

- a matrix is a second order tensor, and (d)PCA is a dimensionality reduction on second order tensors 
- TCA is a dimensionality reduction method on third order tensors 
- previous approaches to dimensionality reduction focus on two modes (tensor jargon for "axes"): neuron and time
- TCA is a way to add more modes to reduce dimensionality: eg. trials

</font>

</div>


</script></section><section data-markdown><script type="text/template">


<div id='left'>

Pros

 - They claim it's a "natural generalization of PCA to higher-order tensors" 
 - unsupservised method 
 - can handle continuous experimental parameters

</div>

<div id ='right'>

Cons 

 - (Lucas thinks they are lying somewhere)
 - linear 
 - not dynamical
 - Joana and Maneesh have a different tensor method 
 
</div>

</script></section><section data-markdown><script type="text/template">

### Latent Factor Analysis via Dynamical Systems (LFADS)

[Pandarinath 2018: Inferring single-trial neural population dynamics using sequential auto-encoders](https://www.nature.com/articles/s41592-018-0109-9)


</script></section><section data-markdown><script type="text/template">

<div id='left'>

Pros

 - dynamical model: you enforce temporal correlation (you don't assume time points are independent)
 - (it is more true)


</div>

<div id ='right'>

Cons 

 - does not demix; only summarise 
 - do we like RNNs?
 
 
</div>


</script></section></section><section ><section data-markdown><script type="text/template">


### Other methods we have no time to talk about 

 - Gaussian Process Factor Analysis 
 - Independent Component Analysis
 - tSNE
 
 
 Good reviews papers:

 - [Cunningham and Byron 2014: Dimensionality reduction for large-scale neural recordings](https://www.nature.com/articles/nn.3776)
 
</script></section><section data-markdown><script type="text/template">

#### tSNE 

<iframe frameborder="0" width="100%" height="500pt" src="https://distill.pub/2016/misread-tsne/"></iframe>


</script></section></section><section  data-markdown><script type="text/template">

### Comparing dimensionality reduction methods


![Dim reduction methods comparison](./notebook/dim_reduction_methods_prop.png) <!-- .element height="70%" width="60%"; style="margin:auto;display:block" -->





<style>

#left {
	margin: 10px 0 15px 20px;
	text-align: left;
	float: left;
	z-index:-10;
	width:48%;
	font-size: 0.85em;
	line-height: 1.5; 
}

#right {
	margin: 10px 0 15px 0;
	float: right;
	text-align: left;
	z-index:-10;
	width:48%;
	font-size: 0.85em;
	line-height: 1.5; 
}

p { text-align: left; }

#myfigure{
	text-align: center;
}
</style>


</script></section></div>
    </div>

    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"transition":"fade"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
